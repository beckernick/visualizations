max.words=100, random.order=FALSE, rot.per=.15, colors = pal2)
dev.off()
jpeg("wordcloud_fed_tweets.jpeg", width=1280,height=800)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 3,
max.words=100, random.order=FALSE, rot.per=.15, colors = pal2)
dev.off()
pal2 <- brewer.pal(8,"Set1")
jpeg("wordcloud_fed_tweets.jpeg", width=1280,height=800)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 3,
max.words=100, random.order=FALSE, rot.per=.15, colors = pal2)
dev.off()
jpeg("wordcloud_fed_tweets.jpeg", width=1280,height=800)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 3,
max.words=100, random.order=FALSE, rot.per=.15, colors = Spectral)
dev.off()
jpeg("wordcloud_fed_tweets.jpeg", width=1280,height=800)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 3,
max.words=100, random.order=FALSE, rot.per=.15, colors = Spectral)
dev.off()
jpeg("wordcloud_fed_tweets.jpeg", width=1280,height=800)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 3,
max.words=100, random.order=FALSE, rot.per=.15, colors = brewer.pal(Spectral))
dev.off()
jpeg("wordcloud_fed_tweets.jpeg", width=1280,height=800)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 3,
max.words=100, random.order=FALSE, rot.per=.15, colors = brewer.pal("Spectral"))
dev.off()
jpeg("wordcloud_fed_tweets.jpeg", width=1280,height=800)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 3,
max.words=100, random.order=FALSE, rot.per=.15, colors = brewer.pal(n = 8, "Spectral"))
dev.off()
inspect(fed_corpus)
tdm = TermDocumentMatrix(fed_corpus, control = list(wordLengths = c(1, Inf)))
tdm
tdm2 = as.matrix(tdm)
str(tdm2)
frequency = colSums(tdm2)
head(frequency)
frequency = sort(frequency, decreasing = TRUE)
head(frequency)
words = names(frequency)
head(words)
fed_tweets = searchTwitter("#federalreserve", n = 1500)
fed_tweets = strip_retweets(fed_tweets, strip_manual = TRUE, strip_mt = TRUE)
# convert to dataframe
fed_df = twListToDF(fed_tweets)
head(fed_df)
fed_text = fed_df$text
head(fed_text)
# convert to utf-8
# remove parts of tweets with unrecognizable characters
fed_text = iconv(fed_text, to = "utf-8-mac", sub = "")
fed_corpus = Corpus(VectorSource(fed_text))
fed_corpus = tm_map(fed_corpus, stripWhitespace)
fed_corpus = tm_map(fed_corpus, content_transformer(tolower))
fed_corpus = tm_map(fed_corpus, removeWords, stopwords("english"))
fed_corpus = tm_map(fed_corpus, removeNumbers)
fed_corpus = tm_map(fed_corpus, removePunctuation)
pal2 <- brewer.pal(8,"Set1")
jpeg("wordcloud_fed_tweets.jpeg", width=1280,height=800)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 3,
max.words=100, random.order=FALSE, rot.per=.15, colors = brewer.pal(n = 8, "Spectral"))
dev.off()
install.packages(packageName, repos = "http://www.omegahat.org/R", type = "source")
install.packages(packageName = "RNYTimes", repos = "http://www.omegahat.org/R", type = "source")
install.packages("RNYTimes", repos = "http://www.omegahat.org/R",
type = "source")
library(RNYTimes)
?RNYTimes
what= paste("by-date", format(Sys.time(), "%Y-%m-%d"),sep="/")
what
library(XML)
library(tm)
library(wordcloud)
library(RColorBrewer)
?community
article.search = "f767c43b72dbc3d36e70af12a5d41baa:5:19607364"
devtools::install_github("ropengov/rtimes")
library(rtimes)
nytimes_cg_key = "c8e99f35730912db808b9ad126120a99:15:19607364"
nytimes_as_key = "f767c43b72dbc3d36e70af12a5d41baa:5:19607364"
nytimes_cf_key = "1b8bf9bc53646606a457f94d1ee4d8db:17:19607364"
nytimes_geo_key = "8ad2a16b2eefa71d10bf67748bf05938:5:19607364"
?cg_rollcallvote
?as_search
fed = as_search(q = "federal reserve", begin_date = "20150719", end_date = "20150819")
fed = as_search(q = "federal reserve", begin_date = "20150719", end_date = "20150819",
key = nytimes_as_key)
head(fed)
fed_mentions = fed$data
fed = as_search(q = "federal reserve", begin_date = "20150719", end_date = "20150819",
key = nytimes_as_key, fl = source)
fed = as_search(q = "federal reserve", begin_date = "20150719", end_date = "20150819",
key = nytimes_as_key, fl = "source")
fed$data[1]
str(fed)
fed = as_search(q = "federal reserve", begin_date = "20150719", end_date = "20150819",
key = nytimes_as_key, fl = "blog")
str(fed)
fed$data[1]
head(fed$data)
fed$data[[1]]
fed = as_search(q = "federal reserve", begin_date = "20150719", end_date = "20150819",
key = nytimes_as_key, fl = "byline")
str(fed)
head(fed$data)
fed = as_search(q = "federal reserve", begin_date = "20150719", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph")
head(fed)
head(fed$data$docs)
fed_leads = unlist(fed$data$docs)
fed_leads
length(fed$data$docs)
fed_vector = vector()
fed$data$docs[[1]]$lead_paragraph
fed$data$docs[[2]]$lead_paragraph
fed_vector = vector()
for (i in 1:length(fed$data$docs)) {
fed_vector[i] = fed$data$docs[[i]]$lead_paragraph
}
fed_vector
fed_leads = vector()
for (i in 1:length(fed$data$docs)) {
fed_leads[i] = fed$data$docs[[i]]$lead_paragraph
}
fed_corpus = Corpus(VectorSource(fed_leads))
fed_corpus = tm_map(fed_corpus, stripWhitespace)
fed_corpus = tm_map(fed_corpus, content_transformer(tolower))
fed_corpus = tm_map(fed_corpus, removeWords, stopwords("english"))
fed_corpus = tm_map(fed_corpus, removeNumbers)
fed_corpus = tm_map(fed_corpus, removePunctuation)
library(tm)
library(wordcloud)
library(RColorBrewer)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 3, max.words=100,
random.order=FALSE, rot.per=.15, colors = brewer.pal(n = 8, "Spectral"))
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 2, max.words=100,
random.order=FALSE, rot.per=.15, colors = brewer.pal(n = 8, "Spectral"))
fed = as_search(q = "federal reserve", begin_date = "20140819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph")
str(fed)
fed_leads = vector()
for (i in 1:length(fed$data$docs)) {
fed_leads[i] = fed$data$docs[[i]]$lead_paragraph
}
fed_corpus = Corpus(VectorSource(fed_leads))
fed_corpus = tm_map(fed_corpus, stripWhitespace)
fed_corpus = tm_map(fed_corpus, content_transformer(tolower))
fed_corpus = tm_map(fed_corpus, removeWords, stopwords("english"))
fed_corpus = tm_map(fed_corpus, removeNumbers)
fed_corpus = tm_map(fed_corpus, removePunctuation)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 2, max.words=100,
random.order=FALSE, rot.per=.15, colors = brewer.pal(n = 8, "Spectral"))
jpeg("wordcloud_fed_leads_nytimes.jpeg", width=1280,height=800)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 2, max.words=100,
random.order=FALSE, rot.per=.15, colors = brewer.pal(n = 8, "Spectral"))
dev.off()
dtm = DocumentTermMatrix(fed_corpus)
inspect(dtm[5:10, 1:5])
findFreqTerms(dtm, 5)
findFreqTerms(dtm, 4)
fed = as_search(q = "federal reserve", begin_date = "20130819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph")
str(fed)
head(fed$data$docs)
fed_leads = vector()
for (i in 1:length(fed$data$docs)) {
fed_leads[i] = fed$data$docs[[i]]$lead_paragraph
}
fed_corpus = Corpus(VectorSource(fed_leads))
fed_corpus = tm_map(fed_corpus, stripWhitespace)
fed_corpus = tm_map(fed_corpus, content_transformer(tolower))
fed_corpus = tm_map(fed_corpus, removeWords, stopwords("english"))
fed_corpus = tm_map(fed_corpus, removeNumbers)
fed_corpus = tm_map(fed_corpus, removePunctuation)
dtm = DocumentTermMatrix(fed_corpus)
inspect(dtm[5:10, 1:5])
findFreqTerms(dtm, 3)
fed = as_search(q = "federal reserve", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph")
fed_leads = vector()
for (i in 1:length(fed$data$docs)) {
fed_leads[i] = fed$data$docs[[i]]$lead_paragraph
}
fed_corpus = Corpus(VectorSource(fed_leads))
fed_corpus = tm_map(fed_corpus, stripWhitespace)
fed_corpus = tm_map(fed_corpus, content_transformer(tolower))
fed_corpus = tm_map(fed_corpus, removeWords, stopwords("english"))
fed_corpus = tm_map(fed_corpus, removeNumbers)
fed_corpus = tm_map(fed_corpus, removePunctuation)
dtm = DocumentTermMatrix(fed_corpus)
inspect(dtm[5:10, 1:5])
findFreqTerms(dtm, 3)
findFreqTerms(dtm, 2)
findFreqTerms(dtm, 1)
fed = as_search(q = "federal reserve", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "snippet")
str(fed)
fed_leads[1]
fed = as_search(q = "federal reserve", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph")
fed_leads = vector()
for (i in 1:length(fed$data$docs)) {
fed_leads[i] = fed$data$docs[[i]]$lead_paragraph
}
fed_leads[1]
fed = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph")
fed$data$docs
fed$data$docs[[11]]
str(fed)
?as_search
fed$data$docs
fed = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = 1)
fed$data$docs
fed = list()
fed = list()
for (i in 0:100) {
fed[[i]] = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = i)
}
fed = list()
for (i in 1:11) {
for (j in 1:11) {
fed[[i]][j] = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = i-1)
}
}
fed = list()
for (i in 1:11) {
for (j in 1:11) {
fed[[i]][j] = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = i)
}
}
fed = list()
for (i in 1:11) {
for (j in 1:10) {
fed[[i]][j] = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = i)
}
}
fed = list()
for (i in 1:11) {
for (j in 1:9) {
fed[[i]][j] = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = i)
}
}
feds = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = 1)
head(feds$data$docs)
feds$data$docs[[1]]$lead_paragraph
fed = list()
for (i in 1:10) {
temp = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = i)
fed = paste(fed, temp$data$docs[[1]]$lead_paragraph)
}
str(fed)
fed = data.frame
fed = data.frame
fed
for (i in 1:10) {
fed_temp = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = i)
fed[,i] = fed_temp$data$docs[[1]]$lead_paragraph
}
fed = as_search(q = "yellen", begin_date = "19990819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = i)
}
str(fed)
head(fed)
head(feds$data$docs)
fed_leads = vector()
for (i in 1:length(fed$data$docs)) {
fed_leads[i] = fed$data$docs[[i]]$lead_paragraph
}
fed_corpus = Corpus(VectorSource(fed_leads))
fed_corpus = tm_map(fed_corpus, stripWhitespace)
fed_corpus = tm_map(fed_corpus, content_transformer(tolower))
fed_corpus = tm_map(fed_corpus, removeWords, stopwords("english"))
fed_corpus = tm_map(fed_corpus, removeNumbers)
fed_corpus = tm_map(fed_corpus, removePunctuation)
dtm = DocumentTermMatrix(fed_corpus)
findFreqTerms(dtm, 1)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 1, max.words=100,
random.order=FALSE, rot.per=.15, colors = brewer.pal(n = 8, "Spectral"))
jpeg("wordcloud_fed_leads_nytimes.jpeg", width=1280,height=800)
wordcloud(fed_corpus, scale=c(3,.5), min.freq = 1, max.words=100,
random.order=FALSE, rot.per=.15, colors = brewer.pal(n = 8, "Spectral"))
dev.off()
fed_data = as_search(q = "federal reserve", begin_date = "20140819", end_date = "20150819",
key = nytimes_as_key, fl = "lead_paragraph", page = i)
str(fed_data)
fed_data$meta
fed_data = as_search(q = "federal reserve", begin_date = "20140819", end_date = "20150819",
key = nytimes_as_key, fl = "word_count", page = i)
str(fed_data)
fed_data$meta$hits
years = seq(1990:2015, by =1)
years = seq(1990, 2015, by =1)
years
hits_vector = vector()
hits_vector = vector()
for (i in years) {
fed_data = as_search(q = "federal reserve", begin_date = paste0(as.character(i),"0819"),
end_date = paste0(as.character(i),"0819"), key = nytimes_as_key, fl = "lead_paragraph")
hits_vector[i] == fed_data$meta$hits
}
hits_vector
for (i in years) {
fed_data = as_search(q = "federal reserve", begin_date = paste0(as.character(i),"0819"),
end_date = paste0(as.character(i),"0819"), key = nytimes_as_key, fl = "lead_paragraph")
#hits_vector[i] == fed_data$meta$hits
print(fed_data$meta$hits)
}
for (i in years) {
fed_data = as_search(q = "federal reserve", begin_date = paste0(as.character(i+1),"0819"),
end_date = paste0(as.character(i),"0819"), key = nytimes_as_key, fl = "lead_paragraph")
#hits_vector[i] == fed_data$meta$hits
print(fed_data$meta$hits)
}
for (i in years) {
fed_data = as_search(q = "federal reserve", begin_date = paste0(as.character(i),"0819"),
end_date = paste0(as.character(i+1),"0819"), key = nytimes_as_key, fl = "lead_paragraph")
#hits_vector[i] == fed_data$meta$hits
print(fed_data$meta$hits)
}
hits_vector = vector()
for (i in years) {
fed_data = as_search(q = "federal reserve", begin_date = paste0(as.character(i),"0819"),
end_date = paste0(as.character(i+1),"0819"), key = nytimes_as_key, fl = "lead_paragraph")
hits_vector[i] == fed_data$meta$hits
#print(fed_data$meta$hits)
}
hits_vector
for (i in years) {
fed_data = as_search(q = "federal reserve", begin_date = paste0(as.character(i),"0819"),
end_date = paste0(as.character(i+1),"0819"), key = nytimes_as_key, fl = "lead_paragraph")
hits_vector[i] == fed_data$meta$hits
#print(fed_data$meta$hits)
}
for (i in years) {
fed_data = as_search(q = "federal reserve", begin_date = paste0(as.character(i),"0819"),
end_date = paste0(as.character(i+1),"0819"), key = nytimes_as_key, fl = "lead_paragraph")
#hits_vector[i] == fed_data$meta$hits
print(fed_data$meta$hits)
}
hits_vector = vector()
for (i in years) {
fed_data = as_search(q = "federal reserve", begin_date = paste0(as.character(i),"0819"),
end_date = paste0(as.character(i+1),"0819"), key = nytimes_as_key, fl = "lead_paragraph")
print(fed_data$meta$hits)
hits_vector[i-1989] == fed_data$meta$hits
}
hits_vector = numeric()
for (i in years) {
fed_data = as_search(q = "federal reserve", begin_date = paste0(as.character(i),"0819"),
end_date = paste0(as.character(i+1),"0819"), key = nytimes_as_key, fl = "lead_paragraph")
print(fed_data$meta$hits)
hits_vector[i-1989] == fed_data$meta$hits
}
library(rnytimes)
library(RNYTimes)
?RNYTimes
nytimes_as_key = "f767c43b72dbc3d36e70af12a5d41baa:5:19607364"
arts = searchArticles("federal reserve", begin_date = "20150820", end_date = "20150820",
key = nytimes_as_key)
arts = searchArticles("federal reserve", begin_date = "20150820", end_date = "20150820",
key = nytimes_as_key, fields = "body")
arts = searchArticles("federal reserve", begin_date = "20150820", end_date = "20150820",
key = nytimes_as_key, fields = "org_facet")
text = readLines("http://www.italaw.com/sites/default/files/laws/italaw6187(6).pdf")
warnings()
library(rvest)
text = readLines("http://www.presidency.ucsb.edu/ws/index.php?pid=108031")
head(text)
head(text, 10)
head(text, 15)
head(text)
text
obama_speech = html(http://www.presidency.ucsb.edu/ws/index.php?pid=108031)
obama_speech = html("http://www.presidency.ucsb.edu/ws/index.php?pid=108031")
str(obama_speech)
obama_speech %>%
html_node("/html/body/table/tbody/tr[2]/td/table/tbody/tr/td[2]/table[3]/tbody/tr[3]/td/span") %>%
html_text()
?html_node
obama_speech %>%
html_node(xpath = "/html/body/table/tbody/tr[2]/td/table/tbody/tr/td[2]/table[3]/tbody/tr[3]/td/span") %>%
html_text()
obama_speech %>%
html_nodes(td) %>%
html_text()
obama_speech %>%
html_nodes("td span") %>%
html_text()
speech_2015 = obama_speech %>%
html_nodes("td span") %>%
html_text()
speech_2015 = speech_2015[7]
speech_2015
str(speech_2015)
speech_2015_small = as.vector(speech_2015)
library(C50)
data(churn)
head(churn)
churn = data(churn)
head(churn)
data(churn)
library(C50)
data(churn)
str(churnTrain)
head(churnTrain)
?churn
str(churnTrain)
churnTrain = churnTrain[, !names(churnTrain) %in% c("state", "area_code", "account_length")]
str(churnTrain)
data(churn)
str(churnTrain)
set.seed(2)
index = sample(1:2, nrow(churnTrain), replace = TRUE, prob = c(0.7, 0.3))
train = churnTrain[index == 1, ]
validation = churnTrain[index == 2, ]
library(caret)
library(xgboost)
?randomforest
?randomForest
?xgboot
?xgboost
library(randomForest)
?randomForest
str(train)
churn.rf = randomForest(formula = churn ~ ., data = train, importance = TRUE)
churn.rf
churn.prediction = predict(churn.rf, validation)
table(churn.prediction, validation$churn)
plot(churn.rf)
importance(churn.rf)
varImpPlot(churn.rf)
hist(margins.rf, main = "margins of random forest for churn data")
margins.rf = margin(churn.rf, trainset)
plot(margins.rf)
str(train)
churnTrain = churnTrain[, !names(churnTrain) %in% c("state", "area_code", "account_length")]
str(churnTrain)
# split the data into a training and validaiton set
set.seed(2)
index = sample(1:2, nrow(churnTrain), replace = TRUE, prob = c(0.7, 0.3))
train = churnTrain[index == 1, ]
validation = churnTrain[index == 2, ]
# random forest
churn.rf = randomForest(formula = churn ~ ., data = train, importance = TRUE)
churn.rf
# split the data into a training and validaiton set
set.seed(2)
index = sample(1:2, nrow(churnTrain), replace = TRUE, prob = c(0.7, 0.3))
train = churnTrain[index == 1, ]
validation = churnTrain[index == 2, ]
# random forest
churn.rf = randomForest(formula = churn ~ ., data = train, importance = TRUE)
churn.rf
# predictions
churn.prediction = predict(churn.rf, validation)
table(churn.prediction, validation$churn)
plot(churn.rf)
importance(churn.rf)
varImpPlot(churn.rf)
?confusionMatrix
confusionMatrix(churn.prediction, validation$churn)
library(h2o)
library(h2o)
localh2o = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '4g')
train.h2o = as.h2o(train, localh2o)
validation.h2o = as.h2o(validation, localh2o)
?h2o.deeplearning
str(train.h2o)
colnames(train.h2o)
model.dl = h2o.deeplearning(x = 1:16, y = 17, training_frame = train.h2o,
validation_frame = validation.h2o)
summary(model.dl)
summary(churn.rf)
model.rf = h2o.randomforest(x = 1:16, y = 17, training_frame = train.h2o,
validation_frame = validation.h2o, ntrees = 500)
model.rf = h2o.randomForest(x = 1:16, y = 17, training_frame = train.h2o,
validation_frame = validation.h2o, ntrees = 500)
summary(model.rf)
?randomForest
churn.rf = randomForest(formula = churn ~ ., data = train, importance = TRUE, mtry = 6)
churn.rf
churn.rf = randomForest(formula = churn ~ ., data = train, importance = TRUE, mtry = 5)
churn.rf
churn.rf = randomForest(formula = churn ~ ., data = train, importance = TRUE, mtry = 4)
churn.rf
churn.rf = randomForest(formula = churn ~ ., data = train, importance = TRUE, mtry = 7)
churn.rf
churn.rf = randomForest(formula = churn ~ ., data = train, importance = TRUE, mtry = 8)
churn.rf
# random forest
churn.rf = randomForest(formula = churn ~ ., data = train, importance = TRUE, mtry = 7)
churn.rf
summary(churn.rf)
# predictions
churn.prediction = predict(churn.rf, validation)
table(churn.prediction, validation$churn)
confusionMatrix(churn.prediction, validation$churn)
system("convert -delay 45 *.jpeg Wordcloud_SOTU_gif.mpg")
setwd("/Users/nickbecker/Documents/R Workspace/Wordcloud SOTU")
system("convert -delay 45 *.jpeg Wordcloud_SOTU_gif.mpg")
